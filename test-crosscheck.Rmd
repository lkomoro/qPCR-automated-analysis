---
title: "test-crosscheck"
author: "Lisa Komoroske"
date: "May 21, 2017"
output: html_document
---

#Microhaplot Exploration for STX GT-Seq primer design
_Notes last updated May 17,2017_

https://github.com/ngthomas/microhaplot

- Installed R packages according to instructions (see link above)
- Walking through example tutorial/vignette data provided
- Good documentation on github readme, and in vignette partner documentation
- I downloaded as PDF and highlighted/commented; see file:
    _"A First Walk Through haPLOType_LKhighlights"_

Summary:
- Makes sense to me how to find the SNPs (with my samtools/angsd code-see Rapture data analyses notes as well as Phil's chapter methods)
- And then downstream how to select loci using the Shiny ap/functions etc
- But what need to work out is intermediate step-creating input VCF files with info of variants to input into microhaplot

- I emailed Thomas and Eric to clarify their criteria and process; Eric's response:
_"I am glad that you find the documentation for microhaplot helpful.  We are still trying to put it together, and we are missing some good, small, easy example vcf and sam files for people to interact with (and for us to show some simple examples).  Thomas is putting some of those together.  

All of the work we have done with microhaplot has been on amplicon sequencing data in which the the reference genome has already been chopped up into a lot of different very short fragments which match (almost exactly) the 96 amplicons we are shooting for.  The program is able to  figure out which "variants go together" because each of the sequences overlapping them is a complete read (of about 100 bp) off the machine.   Dealing with RAD data would be a little harder, especially when it has been aligned to a genome (and not to short "stacks").  To use microhaplot with RAD data I should think you would have to align all those primary (i.e. not the paired end reads, if you are doing paired end, since they all stop and start at different places) reads that are starting from the RAD recognition site and going out 100 bp or so.  Define a reference genome that includes all of these regions as short fragments, each roughly the length of a read and each named as a different chromosome, then align all those in a sam file, call SNPs in them, and use the VCF.  Then the CHROM field in the VCF is the same for SNPs that should appear together on the same read.  

microhaplot is not designed to try to infer haplotypes using reads that start in different places and overlap along  large-ish regions of the genome.  It is made for simply grabbing short reads that all align to a reference that includes a smallish number (100s to a few 1000s) of reference sequences that are about in the size range of the reads."_

From this, trying a few ways to move forward...
*Choices: (should produce similar results?)*
-"Rapture_reference.fasta" has all our 2007 bait loci listed with unique 'locus' names and then 120 bait sequences
-Aligned all the R1s to this and have sorted .bam files
-Note that all of these approaches using the R1s this will not have duplicates removed, so overestimating coverage

1. perform SNP disco using the bam files listed above (angsd and/or Phils workflow), generate VCF from that
    - pro: uses files I already have, with generally updated programs (BWA and samtools updated; bcftools needs new version installed)
    - con: need to check if this is making the appropriate VCF format file
  A. Using samtools online tutorial instructions (mirrored in Phil's chapter)
      References:
            https://github.com/samtools/bcftools/wiki/HOWTOs#mpileup-calling
            https://github.com/PAMorin/SNPdiscovery Explanations of steps in: 'SNP discovery from genomic assemblies_Revised_191216.docx'

    ##### Call variants and output to vcf:
    (note-this errors out with nohup)
    using bam files generated previously to target, R1s, STX only moved to working directory:
    /home/lkomoroske/Dcor_Rapture/Rapture_baitref_sortbam/STX_baitref_bam/
    ```/usr/local/samtools-1.3/bin/samtools mpileup -uf ../../Rapture_baits_reference/Rapture_reference.fasta *.bam | /usr/local/bcftools-1.2/bcftools call -mv -Oz -o STX_baitref_calls.vcf.gz
    ```
1a. Output: This vcf file looks like what we want, but need to learn to interpret columns, figure out what format microhaplot needs...
See interpretation/VCF explanation here:
http://gatkforums.broadinstitute.org/gatk/discussion/1268/what-is-a-vcf-and-how-should-i-interpret-it

    ##### If aligned to related species, can also extract the consensus sequences of the target-species contigs from the alignment so that discovered SNPs will vary within population, and not fixed between your species and the reference species:   
    ```nohup /usr/local/samtools-1.3/bin/samtools mpileup -uf ../../Rapture_baits_reference/Rapture_reference.fasta *.bam | /usr/local/bcftools-1.2/bcftools call -c | vcfutils.pl vcf2fq > Dcor_conscensus_newseq_Rapturebaits.fq &
    ```
    ##### Convert the output fastq file to a fasta file using SEQTK to use as the new reference sequence, and index using ‘bwa index’ as above _(but note this code was for low whole genome coverage vs. capture data like mine)_
    ```seqtk seq -a Dcor_conscensus_newseq_Rapturebaits.fq > Dcor_conscensus_newseq_Rapturebaits.fa
    ```

    ##### Use conscensus sequence as reference to start process over - Align R1 fastq files to new Dcor conscensus
    - Align: Edited *(bwa: Rapture_bwa_baitref.sh)* (STX R1 fastq.gz files into sep folder for source files-240 total)
    - Sam to bam/sort: Edited *(samtools: Rapture_StB_sort.sh)* (deleted sam files after done)
    - Repeat pileup, variant calling, and output to vcf from above:
    ```/usr/local/samtools-1.3/bin/samtools faidx Dcor_conscensus_newseq_Rapturebaits.fa #index new ref with samtools first

    /usr/local/samtools-1.3/bin/samtools mpileup -uf ../Dcor_conscensus_newseq_Rapturebaits.fa *.bam | /usr/local/bcftools-1.2/bcftools call -mv -Oz -o STX_DC_new_conscensus_calls.vcf.gz
    ```
1b. Output: same as 1a-compare the two also to see how this affects it
_output file to compare: /home/lkomoroske/Rapture_microhap_scratch/STX_baitref_calls.vcf.gz_
    (also downloaded to STX_microhaplotype development folder)

2. Run Eric's NGS pipeline
  - pro: should output all the files I need automatically (e.g., conscensus sequences and merged vcf input file)
  - con: hasn't been updated recently, and not sure what's happening at certain steps if my data are compatible for legit results, etc., uses old version of samtools etc.; note need to change file names so unique identifier comes first (following how typically have it with the z#)
  - Copied STX R1 fastq.gz file into sep folder for source files-240 total
  - Edited ngs-pipeline R script and created directories/reference sequence as instructed in "SNP discovery and genotypes from NGS database_af notes_LK highlights.docx"
  ```nohup Rscript ./ngs-snp-pipeline-Rap-micro051617-v3.6.R &
  ```
_output file to compare: /home/lkomoroske/Rapture_microhap_scratch/Rapture-STX_snps_20170516_1515/vcf/Rapture-STX_snps_20170516_1515.merged.vcf_
    (also downloaded to STX_microhaplotype development folder)

##Comparisons of output of 1a,1b, and 2: (see worked excels of vcf's with extracted values,etc)
1. a. (see above) Rapture baits reference, R1's only aligned, SNP disco for STX samples=1937 of the baits have variants, 7704 SNPs total identified
    - Extracting some of the more important values (DP and calculated AF/MAF), drops precipitously-for example, with a filter of DP>50, MAF>0.05 have 354 candidate SNPs, which occur in unique 301 loci.
    - So, this may be worth exploring in microhaplot and/or using for design for straight GT-Seq, but there aren't a large number of fragments with multiple SNPs- i.e., this data approach may not provide enough sites for powerful microhap development
    - This isn't super surprising because using only the R1's, we only have ~85 base fragments, many of which we previously screened for allowing only one variant (in Rapture design)
1. b. (see above) New consensus reference, R1's only aligned, SNP disco for STX samples=412 of the baits have variants, 610 SNPs total identified
    - Teasing apart some basic metrics from the vcf files, it's obvious that 1b has issues; of the 610 possible variants, only 38 of them have MAF>=0.05, and a subset of those have low DP (depth of raw reads), so have low confidence. And for the small subset of potential good SNPs (higher coverage and MAF) that are identified here, they should also come up in 1a. (hard to check directly because the positions are not equal between the two references to compare)
_Conclusion: for now discontinuing exploring option 1b._
2.    Eric's pipeline, Rapture baits reference, R1's only aligned, SNP disco for STX samples = 1589 of the baits have variants, 4492 SNPs total identified
  - Looking at the merged.vcf file and the snp.base.freq files, something wonky is happening. Though ID'ed large # of possible variants, only a handful even are being evaluated as heterozygous. This is really weird:
```grep -o 0/1 Rapture-STX_snps_20170516_1515.merged.vcf| wc -l
#  65 occurrences
grep -o 0/0 Rapture-STX_snps_20170516_1515.merged.vcf | wc -l
#   27552 occurrences
grep -o 1/1 Rapture-STX_snps_20170516_1515.merged.vcf | wc -l
#  487153 occurrences
```
- Looking at the DP, sorting it, the highest DoC is 217; compare that to 35502 for the max coverage in 1a. Looking back at some capture array VCF's, have higher #'s for DP, so something in the way my data is being not matching with the pipeline functions is causing something weird to happen-perhaps so there's only a handful of reads per sample/locus, making it less likely to call a het?
- Also ran through merged vcf to genotype R script just to see; can't load the total file in excel, but if parse and sort the total reads column see that only small # have more than 1 read, a few have 2, one has 3. Obviously there's an issue here.
- - Also, none of them have LABIDs, but that can probably be easily fixed with formatting of file names (mine didn't conform to our typical archive formatting)
- Talked to Eric-there are filters in the pipeline, like removing duplicates-this is likely based on removing ones that all start at the same position, which would be an issue for my data because they are all starting at the cut site
- _Eric is going to check the pipleline and see if he can pull out the code that inputs the vcf to output the snp.base.freq file so I can run that with my input vcf from 1a, etc. Also, he is going to look for the script to generate the new consensus sequence_
```wc -l Rapture-STX_snps_20170516_1515.merged.vcf.genotypes.csv
 #1078081
cut -d "," -f11 Rapture-STX_snps_20170516_1515.merged.vcf.genotypes.csv |sort|tail
```

3. Using ANGSD, similar to how we did SNP discovery on our leatherback RAD data...
a. using file generated from mapping to genome (PE)
    *see Dcor_rapture_analysis_notes file for upstream work that generated, selected this input file*
    -Use **DC_STX.MAFsorted.mafs.gz** as source file
    -Has 8112 lines (=potential SNPs)
    -was generated from:
  ```  nohup angsd -bam DC_STX_bamlist -out DC_STX  -minQ 20 -minMapQ 10 -minInd 50 -GL 1 -doMajorMinor 1 -doMaf 2 -SNP_pval 1e-6 -minMaf 0.05  &
  ```
  Making vcf file from same starting bam files: *note that the dovcf is a wrapper for other functions so will remake mafs files and possibly can tweak to call genotypes, etc; make sure to change output name accordinly to not overwrite*
  ```nohup angsd -bam DC_STX_bamlist -out DC_STXv -dovcf 1 -GL 1 -dopost 1 -domajorminor 1 -doMaf 2 -SNP_pval 1e-6 -minMaf 0.05  -minQ 20 -minMapQ 10 -minInd 50 &
  zless DC_STXv.vcf.gz |wc -l
  #8117
  ```
  - So it looks 'right', but seems to only output likelihoods per sample...angsd documentation for this says this is still beta
  - Also doesn't label individuals IDs (but for my purposes here I don't really need that)
  - Since -doVcf is just a wrapper, try giving it the command to call genotypes and it will put them in there:
  ```nohup angsd -bam DC_STX_bamlist -out DC_STXv2 -dovcf 1 -minQ 20 -minMapQ 10 -minInd 50 -GL 1 -doMajorMinor 1 -doMaf 2 -SNP_pval 1e-6 -minMaf 0.05 -doGeno 13 -doPost 2  -postCutoff 0.9 &
  ```
  - Result appears the same as far as the type of information output into the vcf (just the location, REF/ALT alleles and then the genotype probabilities and scaled genotype likelihoods for each sample). However, the # of potential SNPs drops to 1779 (compared to 8111 above bc more stringent filtering-_see below_).

b. Run as 3a, but just for the target regions:
  ```nohup angsd -bam DC_STX_bamlist -out DC_STX_TRv1 -dovcf 1 -minQ 20 -minMapQ 10 -minInd 50 -GL 1 -doMajorMinor 1 -doMaf 2 -SNP_pval 1e-6 -minMaf 0.05 -doPost 2 -rf /home/lkomoroske/Dcor_Rapture/Rapture_scripts_and_metadata_keys/Rapture_sitekey_2017-14-02_forANGSD_400bptargets.txt &
  ```
  1675 potential SNPs

  ```nohup angsd -bam DC_STX_bamlist -out DC_STX_TRv2 -dovcf 1 -minQ 20 -minMapQ 10 -minInd 50 -GL 1 -doMajorMinor 1 -doMaf 2 -SNP_pval 1e-6 -minMaf 0.05 -doGeno 13 -doPost 2  -postCutoff 0.9 -rf /home/lkomoroske/Dcor_Rapture/Rapture_scripts_and_metadata_keys/Rapture_sitekey_2017-14-02_forANGSD_400bptargets.txt &
  ```
  1033 potential SNPs

  - Comparing the two lists for the TRs, indeed it is getting rid of sites that have too limited data to be calling GT's for large proportion of samples (e.g., the probabilities for each of the genotypes is 0.33,0.33,0.33). All of the sites in the more stringent list are in the larger one (ran simple vlookup cross check)
  - Made some simple filters to ID fragments that have other SNPs within 50 bases, named them as the same 'fragment'
  - Will likely need to be careful about using the ones that have 3+ SNPs in a row-could be misalignment, etc.

c. Call variants and output to vcf using code from 1a., but on STX PE bam files mapped to genome
  ```xargs -a DC_STX_bamlist cp -t ./STX_cpbam/ #copy to temp folder-delete after to avoid dup files running around

  cd STX_cpbam
  /usr/local/samtools-1.3/bin/samtools mpileup -uf /home/lkomoroske/Dcor_Rapture/GigaDB_green_turtle_reference_genome/Cmyd.v1.1.fa *.bam | /usr/local/bcftools-1.2/bcftools call -mv -Oz -o STX_GTgenome_ref_calls.vcf.gz &
  ```
  - This 'worked', except that includes a ton of sites across the entire genome that have just a few reads. Need to filter this to only include sites with a lower limit coverage threshold like:
  ```vcffilter -f "DP > 100" STX_GTgenome_ref_calls.vcf.gz >STX_GTgenome_ref_calls_DPg50.vcf.gz
  ```
  - Need to install vcflib to use the vcffilter function- https://github.com/vcflib/vcflib#installing
  - Emailed Rich asking to install-wait to hear back to continue on this
  - While looking for solutions, came across vcfR R package. Seems to have filtering options and produce nice graphics
  - But whole genome is too much (overloads memory capacity/crashes R)-made to work by chromosome or a 'super chromosome'...come back and explore this perhaps looking at different sections, or with just the target regions
  https://cran.r-project.org/web/packages/vcfR/vignettes/intro_to_vcfR.html
  https://cran.r-project.org/web/packages/vcfR/vcfR.pdf


d. Extracting potential sites identified in 3b. from 3c. vcf ...basically trying to combine positives of 3b. (angsd vcf) and 3c. (samtools vcf) methods to combine filtered sites and be able to get information about genotypes and STX allele frequencies
          - created 'key' file of concatenated scaffold_pos from TRv2
          - added column mirroring that in STX_GTgenome_ref_calls.vcf.gz and then extracting matching lines (tested first):
  ```awk '{print $1 "_" $2 "\t" $0 }' STX_GTgenome_ref_calls-short1.vcf >STX_GTgenome_ref_scafpos.vcf &
  nohup grep -Fwf TR_STX_variant_list.txt SSTX_GTgenome_ref_scafpos.vcf > STX_GTgenome_ref_TRv2list.vcf &
  ```
  _(deleted output file of STX_GTgenome_ref_calls-short1.vcf to save space after)_

- output: 724 potential SNPs
- it seems odd that so many dropped out between the two
- worked up in excel: _STX_GTgenome_ref_TRv2listLK.xlsx_
- **extracting the metrics, if set MAF = 0.05: 553 SNPs on 485 fragments; MAF = 0.1: 445 SNPs on 398 fragments; MAF = 0.2: 302 SNPs on 268 fragments**


e. Extracting potential sites identified in 3a. from 3c. vcf to include more sites
```zcat DC_STXv2.vcf.gz| awk '{print $1 "_" $2 }'|grep -v "#" >DC_STXv2_variant_list
nohup grep -Fwf DC_STXv2_variant_list STX_GTgenome_ref_scafpos.vcf > STX_GTgenome_ref_allregions_v2list.vcf &
```
- output: 1356 potential SNPs (from 1779 in 3a.)
- worked up in excel: _STX_GTgenome_ref_allregions_v2list_LK.xlsx_
- **extracting the metrics, if set MAF = 0.05: 1033 SNPs on x fragments; MAF = 0.1: 779 SNPs on x fragments; MAF = 0.2: 519 SNPs on x fragments**

**Need to do:
1. Identify linked fragments in DC_STXv2_variant_list_workedLK.xlsx following what I did earlier
2. Link those results into STX_GTgenome_ref_allregions_v2list_LK.xlsx so see # fragments for each and insert above
3. cross reference loci in 3d. vs. 3e
4. Create a key file for angsd for just those sites, run and extract genotypes, put into StrataG, run duplicates
5. Create a couple fake duplicate files and redo #4
6. Repeat #5, but by randomly subsampling instead of just duplicating/renaming**
----------------------------------------------------------------------------------------------------------
3f???....generate analogous file with only R1's, mapped to Rapture probe 120base targets
  -Using bam files generated previously to target, R1s, STX only moved to working directory:
    /home/lkomoroske/Dcor_Rapture/Rapture_baitref_sortbam/STX_baitref_bam/
```  nohup angsd -bam DC_STX_baitref_bamlist -out DC_STX_baitref  -minQ 20 -minMapQ 10 -minInd 50 -GL 1 -doMajorMinor 1 -doMaf 2 -SNP_pval 1e-6 -minMaf 0.05  &
zless DC_STX_baitref.mafs.gz
zcat DC_STX_baitref.mafs.gz  | wc -l
```
    - Has 340 lines (=potential SNPs)
    - I find it strange this dropped down so drastically..._think more about what causes this..._
_ditching this..._
